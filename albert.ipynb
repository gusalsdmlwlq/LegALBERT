{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhm9507/anaconda3/envs/convlab/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jhm9507/anaconda3/envs/convlab/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jhm9507/anaconda3/envs/convlab/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jhm9507/anaconda3/envs/convlab/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jhm9507/anaconda3/envs/convlab/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jhm9507/anaconda3/envs/convlab/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jhm9507/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jhm9507/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jhm9507/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jhm9507/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jhm9507/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jhm9507/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertForMaskedLM, AlbertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"classifier_dropout_prob\": 0.1,\n",
       "  \"embedding_size\": 128,\n",
       "  \"eos_token_id\": 3,\n",
       "  \"hidden_act\": \"gelu_new\",\n",
       "  \"hidden_dropout_prob\": 0,\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"inner_group_num\": 1,\n",
       "  \"intermediate_size\": 16384,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"albert\",\n",
       "  \"num_attention_heads\": 64,\n",
       "  \"num_hidden_groups\": 1,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AlbertConfig()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config.hidden_size = 512\n",
    "config.num_attention_heads = 8\n",
    "config.num_hidden_layers = 8\n",
    "config.intermediate_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bert = AlbertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertForMaskedLM(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (ffn_output): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (predictions): AlbertMLMHead(\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (decoder): Linear(in_features=128, out_features=30000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.randint(0, 100, size=(4, 10))\n",
    "labels = torch.ones(4, 10, dtype=torch.int64) * -100\n",
    "pad_mask = (inputs != 0)\n",
    "outputs = bert.forward(inputs, masked_lm_labels=labels, attention_mask=pad_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., grad_fn=<NllLossBackward>),\n",
       " tensor([[[ 0.2386,  0.1013,  0.2942,  ...,  0.1356,  0.5454,  0.0411],\n",
       "          [-0.0868,  0.2550,  0.0346,  ...,  0.0517,  0.7233,  0.0927],\n",
       "          [-0.0600,  0.1326,  0.1952,  ...,  0.1849,  0.4882, -0.0519],\n",
       "          ...,\n",
       "          [-0.1079,  0.0753,  0.0757,  ...,  0.0027,  0.4796, -0.0111],\n",
       "          [ 0.1005,  0.2467, -0.0632,  ...,  0.0754,  0.5828,  0.2176],\n",
       "          [-0.0781,  0.2437,  0.1634,  ...,  0.0342,  0.4159,  0.0381]],\n",
       " \n",
       "         [[ 0.1960,  0.2575,  0.1832,  ...,  0.0406,  0.4451,  0.2719],\n",
       "          [ 0.1068,  0.3842,  0.3521,  ...,  0.0665,  0.7410,  0.1495],\n",
       "          [ 0.0920,  0.2579,  0.3175,  ...,  0.1515,  0.5917,  0.1937],\n",
       "          ...,\n",
       "          [ 0.2496,  0.1999,  0.2087,  ..., -0.0307,  0.6030,  0.0847],\n",
       "          [ 0.3032,  0.1052,  0.2005,  ...,  0.2435,  0.3335,  0.0874],\n",
       "          [ 0.4298,  0.1106,  0.3256,  ..., -0.0884,  0.4034, -0.0322]],\n",
       " \n",
       "         [[ 0.2913,  0.1707,  0.1827,  ..., -0.0680,  0.3658,  0.0925],\n",
       "          [-0.0854,  0.2486,  0.0037,  ...,  0.0708,  0.6708, -0.0212],\n",
       "          [ 0.0125,  0.0411,  0.2803,  ...,  0.0448,  0.4266, -0.0838],\n",
       "          ...,\n",
       "          [-0.0300, -0.1016,  0.1518,  ..., -0.0647,  0.2159, -0.0022],\n",
       "          [ 0.1859,  0.2276, -0.0738,  ..., -0.0281,  0.2344, -0.0307],\n",
       "          [-0.1309, -0.0205,  0.0127,  ..., -0.1090,  0.2558, -0.1540]],\n",
       " \n",
       "         [[ 0.2086,  0.2232,  0.3541,  ...,  0.1344,  0.3788,  0.1571],\n",
       "          [-0.0311,  0.1300,  0.2669,  ...,  0.0755,  0.5931,  0.1663],\n",
       "          [ 0.1058,  0.0649,  0.2048,  ...,  0.1398,  0.5252,  0.0784],\n",
       "          ...,\n",
       "          [ 0.1327,  0.0936,  0.1705,  ...,  0.0018,  0.3818,  0.1020],\n",
       "          [ 0.4430,  0.1204,  0.0907,  ..., -0.0349,  0.4106,  0.1488],\n",
       "          [ 0.0695,  0.2462,  0.2412,  ...,  0.0306,  0.4053, -0.1203]]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convlab",
   "language": "python",
   "name": "convlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
